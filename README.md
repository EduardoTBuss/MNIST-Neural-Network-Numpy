# MNIST-Neural-Network-Numpy - MNIST Digit Classification

## ğŸ¯ Sobre o Projeto

Este projeto foi desenvolvido com o objetivo de **entender profundamente a matemÃ¡tica por trÃ¡s de Machine Learning e Neural Networks**. Ao invÃ©s de usar frameworks de alto nÃ­vel, implementei toda a rede neural do zero usando apenas NumPy, fazendo todas as contas na mÃ£o - forward propagation, backpropagation, cÃ¡lculo de gradientes, etc.

Foi uma jornada incrÃ­vel de aprendizado onde pude realmente compreender o que acontece "por baixo do capÃ´" de modelos de deep learning! ğŸ§ ğŸ“

## ğŸ“‹ DescriÃ§Ã£o

Uma rede neural feedforward de 3 camadas implementada do zero para classificar dÃ­gitos manuscritos do dataset MNIST. A implementaÃ§Ã£o inclui:

- **Arquitetura**: 784 â†’ 64 â†’ 32 â†’ 10 neurÃ´nios
- **FunÃ§Ãµes de ativaÃ§Ã£o**: ReLU (camadas ocultas) e Softmax (saÃ­da)
- **TÃ©cnicas de regularizaÃ§Ã£o**: Dropout e Weight Decay (L2)
- **OtimizaÃ§Ã£o**: Gradient Descent com mini-batches
- **AcurÃ¡cia**: ~97-98% no conjunto de teste

## ğŸš€ Funcionalidades

- âœ… ImplementaÃ§Ã£o manual de forward e backward propagation
- âœ… Dropout para regularizaÃ§Ã£o
- âœ… Weight Decay (L2 regularization)
- âœ… Mini-batch gradient descent
- âœ… InicializaÃ§Ã£o He para pesos
- âœ… VisualizaÃ§Ã£o de prediÃ§Ãµes (corretas e erradas)
- âœ… GrÃ¡ficos de loss durante treinamento

## ğŸ“Š Arquitetura da Rede

```
Input Layer:    784 neurÃ´nios (28x28 pixels)
                  â†“
Hidden Layer 1:  64 neurÃ´nios (ReLU + Dropout)
                  â†“
Hidden Layer 2:  32 neurÃ´nios (ReLU + Dropout)
                  â†“
Output Layer:    10 neurÃ´nios (Softmax)
```

## ğŸ› ï¸ Tecnologias Utilizadas

- Python 3.13.2
- NumPy (computaÃ§Ã£o numÃ©rica)
- Matplotlib (visualizaÃ§Ã£o)
- TensorFlow/Keras (apenas para carregar o dataset MNIST)

## ğŸ“¦ InstalaÃ§Ã£o

```bash
# Clone o repositÃ³rio
git clone https://github.com/EduardoTBuss/MNIST-Neural-Network-Numpy
cd MNIST-Neural-Network-Numpy

# Instale as dependÃªncias
pip install -r requirements.txt
```

## ğŸ® Como Usar

```bash
python main.py
```

O script irÃ¡:
1. Carregar e preprocessar o dataset MNIST
2. Treinar a rede neural por 35 Ã©pocas
3. Salvar grÃ¡ficos de loss a cada Ã©poca
4. Exibir a acurÃ¡cia final no conjunto de teste
5. Mostrar exemplos de prediÃ§Ãµes (corretas e incorretas)

## âš™ï¸ HiperparÃ¢metros

```python
LR = 0.006              # Taxa de aprendizado
batch_size = 16         # Tamanho do mini-batch
epochs = 35             # NÃºmero de Ã©pocas
weight_decay = 0.001    # RegularizaÃ§Ã£o L2
dropout1 = 0.1          # Dropout primeira camada
dropout2 = 0.1          # Dropout segunda camada
```

## ğŸ“ˆ Resultados Esperados

- **AcurÃ¡cia de Treinamento**: ~98-99%
- **AcurÃ¡cia de Teste**: ~97-98%
- **Loss Final**: ~0.07-0.10

## ğŸ” Componentes Principais

### Forward Propagation
Calcula as ativaÃ§Ãµes de cada camada:
```
Z1 = W1Â·X + B1
A1 = ReLU(Z1)
Z2 = W2Â·A1 + B2
A2 = ReLU(Z2)
Z3 = W3Â·A2 + B3
A3 = Softmax(Z3)
```

### Backward Propagation
Calcula os gradientes usando a regra da cadeia:
```
dL/dW = (1/m) Â· dZ Â· A^T
dL/dB = (1/m) Â· Î£(dZ)
```

### RegularizaÃ§Ã£o
- **Dropout**: Desativa aleatoriamente neurÃ´nios durante o treino
- **Weight Decay**: Adiciona penalidade L2 aos pesos

## ğŸ“ Estrutura de Arquivos

```
.
â”œâ”€â”€ main.py                # Script principal
â”œâ”€â”€ requirements.txt       # DependÃªncias do projeto
â”œâ”€â”€ grafico_loss.png       # GrÃ¡fico de loss (atualizado a cada Ã©poca)
â”œâ”€â”€ LICENSE                # LicenÃ§a MIT
â””â”€â”€ README.md              # Este arquivo
```

## ğŸ¨ VisualizaÃ§Ãµes

O cÃ³digo gera:
- **GrÃ¡fico de Loss**: Mostra a evoluÃ§Ã£o do loss durante o treinamento
- **Grid 3x3 de PrediÃ§Ãµes**: Mostra imagens com labels verdadeiros e preditos
- **Grid 3x3 de Erros**: Mostra apenas exemplos onde a rede errou

## ğŸ§® O que Aprendi

Implementar tudo do zero me permitiu entender:
- Como funciona a backpropagation matematicamente
- A importÃ¢ncia da inicializaÃ§Ã£o de pesos
- Como o dropout previne overfitting
- O papel da regularizaÃ§Ã£o L2
- Como otimizadores atualizam os parÃ¢metros
- A diferenÃ§a entre gradientes no treino e na inferÃªncia

## ğŸ¤ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para abrir issues ou pull requests.

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para mais detalhes.

---

**â­ Se este projeto te ajudou a entender melhor Neural Networks, considere dar uma estrela!**